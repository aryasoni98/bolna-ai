diff --git a/bolna/providers.py b/bolna/providers.py
index 5d07a27..5001bd6 100644
--- a/bolna/providers.py
+++ b/bolna/providers.py
@@ -1,5 +1,5 @@
 from .synthesizer import PollySynthesizer, ElevenlabsSynthesizer, OPENAISynthesizer, DeepgramSynthesizer, AzureSynthesizer, CartesiaSynthesizer, SmallestSynthesizer, SarvamSynthesizer, RimeSynthesizer
-from .transcriber import DeepgramTranscriber, WhisperTranscriber, AzureTranscriber
+from .transcriber import DeepgramTranscriber, WhisperTranscriber, AzureTranscriber, AssemblyAITranscriber
 from .input_handlers import DefaultInputHandler, TwilioInputHandler, ExotelInputHandler, PlivoInputHandler, DailyInputHandler
 from .output_handlers import DefaultOutputHandler, TwilioOutputHandler, ExotelOutputHandler, PlivoOutputHandler, DailyOutputHandler
 from .llms import OpenAiLLM, LiteLLM
@@ -19,13 +19,15 @@ SUPPORTED_SYNTHESIZER_MODELS = {
 SUPPORTED_TRANSCRIBER_PROVIDERS = {
     'deepgram': DeepgramTranscriber,
     'whisper': WhisperTranscriber,
-    'azure': AzureTranscriber
+    'azure': AzureTranscriber,
+    'assemblyai': AssemblyAITranscriber
 }
 
 #Backwards compatibility
 SUPPORTED_TRANSCRIBER_MODELS = {
     'deepgram': DeepgramTranscriber,
-    'whisper': WhisperTranscriber #Seperate out a transcriber for https://github.com/bolna-ai/streaming-transcriber-server or build a deepgram compatible proxy
+    'whisper': WhisperTranscriber, #Seperate out a transcriber for https://github.com/bolna-ai/streaming-transcriber-server or build a deepgram compatible proxy
+    'assemblyai': AssemblyAITranscriber
 }
 
 SUPPORTED_LLM_PROVIDERS = {
diff --git a/bolna/transcriber/__init__.py b/bolna/transcriber/__init__.py
index 0baae3d..9435aad 100644
--- a/bolna/transcriber/__init__.py
+++ b/bolna/transcriber/__init__.py
@@ -2,3 +2,4 @@ from .base_transcriber import BaseTranscriber
 from .deepgram_transcriber import DeepgramTranscriber
 from .whisper_transcriber import WhisperTranscriber
 from .azure_transcriber import AzureTranscriber
+from .assemblyai_transcriber import AssemblyAITranscriber
diff --git a/bolna/transcriber/assemblyai_transcriber.py b/bolna/transcriber/assemblyai_transcriber.py
new file mode 100644
index 0000000..31192ed
--- /dev/null
+++ b/bolna/transcriber/assemblyai_transcriber.py
@@ -0,0 +1,419 @@
+import asyncio
+import traceback
+import os
+import json
+import aiohttp
+import time
+import websockets
+from websockets.asyncio.client import ClientConnection
+from websockets.exceptions import ConnectionClosedError, InvalidHandshake
+from urllib.parse import urlencode
+from dotenv import load_dotenv
+
+from .base_transcriber import BaseTranscriber
+from bolna.helpers.logger_config import configure_logger
+from bolna.helpers.utils import create_ws_data_packet
+
+logger = configure_logger(__name__)
+load_dotenv()
+
+
+class AssemblyAITranscriber(BaseTranscriber):
+    def __init__(self, telephony_provider, input_queue=None, model='best', stream=True, language="en", 
+                 sampling_rate="16000", encoding="linear16", output_queue=None, keywords=None,
+                 process_interim_results="true", **kwargs):
+        super().__init__(input_queue)
+        self.language = language
+        self.stream = stream
+        self.provider = telephony_provider
+        self.heartbeat_task = None
+        self.sender_task = None
+        self.model = model
+        self.sampling_rate = 16000
+        self.encoding = encoding
+        self.api_key = kwargs.get("transcriber_key", os.getenv('ASSEMBLYAI_API_KEY'))
+        self.assemblyai_host = os.getenv('ASSEMBLYAI_HOST', 'api.assemblyai.com')
+        self.transcriber_output_queue = output_queue
+        self.transcription_task = None
+        self.keywords = keywords
+        self.audio_cursor = 0.0
+        self.transcription_cursor = 0.0
+        self.interruption_signalled = False
+        if not self.stream:
+            self.api_url = f"https://{self.assemblyai_host}/v2/transcript"
+            self.session = aiohttp.ClientSession()
+        self.audio_submitted = False
+        self.audio_submission_time = None
+        self.num_frames = 0
+        self.connection_start_time = None
+        self.process_interim_results = process_interim_results
+        self.audio_frame_duration = 0.0
+        self.connected_via_dashboard = kwargs.get("enforce_streaming", True)
+        #Message states
+        self.curr_message = ''
+        self.finalized_transcript = ""
+        self.final_transcript = ""
+        self.is_transcript_sent_for_processing = False
+        self.websocket_connection = None
+        self.connection_authenticated = False
+
+    def get_assemblyai_ws_url(self):
+        # AssemblyAI real-time streaming endpoint
+        ws_url = f"wss://{self.assemblyai_host}/v2/realtime/ws"
+        
+        # Set audio parameters based on provider
+        if self.provider in ('twilio', 'exotel', 'plivo'):
+            self.encoding = 'mulaw' if self.provider in ("twilio") else "linear16"
+            self.sampling_rate = 8000
+            self.audio_frame_duration = 0.2  # With telephony we are sending 200ms at a time
+        elif self.provider == "web_based_call":
+            self.encoding = "linear16"
+            self.sampling_rate = 16000
+            self.audio_frame_duration = 0.256
+        elif not self.connected_via_dashboard:
+            self.encoding = "linear16"
+            self.sampling_rate = 16000
+            self.audio_frame_duration = 0.5
+
+        if self.provider == "playground":
+            self.sampling_rate = 8000
+            self.audio_frame_duration = 0.0  # There's no streaming from the playground
+
+        return ws_url
+
+    async def send_heartbeat(self, ws: ClientConnection):
+        try:
+            while True:
+                # AssemblyAI doesn't require explicit heartbeat, but we'll send a ping
+                try:
+                    await ws.ping()
+                except ConnectionClosedError as e:
+                    logger.info(f"Connection closed while sending ping: {e}")
+                    break
+                except Exception as e:
+                    logger.error(f"Error sending ping: {e}")
+                    break
+                    
+                await asyncio.sleep(30)  # Send a ping every 30 seconds
+        except asyncio.CancelledError:
+            logger.info("Heartbeat task cancelled")
+            raise
+        except Exception as e:
+            logger.error('Error in send_heartbeat: ' + str(e))
+            raise
+
+    async def toggle_connection(self):
+        self.connection_on = False
+        if self.heartbeat_task is not None:
+            self.heartbeat_task.cancel()
+        if self.sender_task is not None:
+            self.sender_task.cancel()
+        
+        if self.websocket_connection is not None:
+            try:
+                await self.websocket_connection.close()
+                logger.info("Websocket connection closed successfully")
+            except Exception as e:
+                logger.error(f"Error closing websocket connection: {e}")
+            finally:
+                self.websocket_connection = None
+                self.connection_authenticated = False
+
+    async def _get_http_transcription(self, audio_data):
+        if self.session is None or self.session.closed:
+            self.session = aiohttp.ClientSession()
+
+        headers = {
+            'Authorization': f'{self.api_key}',
+            'Content-Type': 'audio/wav'
+        }
+
+        self.current_request_id = self.generate_request_id()
+        self.meta_info['request_id'] = self.current_request_id
+        start_time = time.time()
+        
+        # Upload audio file first
+        upload_url = f"https://{self.assemblyai_host}/v2/upload"
+        async with self.session.post(upload_url, data=audio_data, headers=headers) as upload_response:
+            if upload_response.status != 200:
+                logger.error(f"Upload failed: {upload_response.status}")
+                return None
+            upload_data = await upload_response.json()
+            audio_url = upload_data['upload_url']
+
+        # Submit transcription job
+        transcript_data = {
+            'audio_url': audio_url,
+            'language_code': self.language,
+            'model': self.model
+        }
+        
+        async with self.session.post(self.api_url, json=transcript_data, headers=headers) as response:
+            if response.status != 200:
+                logger.error(f"Transcription submission failed: {response.status}")
+                return None
+            transcript_response = await response.json()
+            transcript_id = transcript_response['id']
+
+        # Poll for completion
+        while True:
+            async with self.session.get(f"{self.api_url}/{transcript_id}", headers=headers) as status_response:
+                if status_response.status != 200:
+                    logger.error(f"Status check failed: {status_response.status}")
+                    return None
+                status_data = await status_response.json()
+                
+                if status_data['status'] == 'completed':
+                    transcript = status_data['text']
+                    self.meta_info["start_time"] = start_time
+                    self.meta_info['transcriber_latency'] = time.time() - start_time
+                    self.meta_info['transcriber_duration'] = status_data.get('audio_duration', 0)
+                    return create_ws_data_packet(transcript, self.meta_info)
+                elif status_data['status'] == 'error':
+                    logger.error(f"Transcription failed: {status_data.get('error', 'Unknown error')}")
+                    return None
+                
+                await asyncio.sleep(1)  # Poll every second
+
+    async def _check_and_process_end_of_stream(self, ws_data_packet, ws):
+        if 'eos' in ws_data_packet['meta_info'] and ws_data_packet['meta_info']['eos'] is True:
+            await self._close(ws, data={"type": "CloseStream"})
+            return True  # Indicates end of processing
+
+        return False
+
+    def get_meta_info(self):
+        return self.meta_info
+
+    async def sender(self, ws=None):
+        try:
+            while True:
+                ws_data_packet = await self.input_queue.get()
+                # If audio submitted was false, that means that we're starting the stream now. That's our stream start
+                if not self.audio_submitted:
+                    self.audio_submitted = True
+                    self.audio_submission_time = time.time()
+                end_of_stream = await self._check_and_process_end_of_stream(ws_data_packet, ws)
+                if end_of_stream:
+                    break
+                self.meta_info = ws_data_packet.get('meta_info')
+                start_time = time.time()
+                transcription = await self._get_http_transcription(ws_data_packet.get('data'))
+                if transcription:
+                    transcription['meta_info']["include_latency"] = True
+                    transcription['meta_info']["transcriber_latency"] = time.time() - start_time
+                    transcription['meta_info']['audio_duration'] = transcription['meta_info']['transcriber_duration']
+                    transcription['meta_info']['last_vocal_frame_timestamp'] = start_time
+                    yield transcription
+
+            if self.transcription_task is not None:
+                self.transcription_task.cancel()
+        except asyncio.CancelledError:
+            logger.info("Cancelled sender task")
+            return
+
+    async def sender_stream(self, ws: ClientConnection):
+        try:
+            while True:
+                ws_data_packet = await self.input_queue.get()
+                # Initialise new request
+                if not self.audio_submitted:
+                    self.meta_info = ws_data_packet.get('meta_info')
+                    self.audio_submitted = True
+                    self.audio_submission_time = time.time()
+                    self.current_request_id = self.generate_request_id()
+                    self.meta_info['request_id'] = self.current_request_id
+
+                end_of_stream = await self._check_and_process_end_of_stream(ws_data_packet, ws)
+                if end_of_stream:
+                    break
+                self.num_frames += 1
+                # save the audio cursor here
+                self.audio_cursor = self.num_frames * self.audio_frame_duration
+                
+                try:
+                    # Send audio data to AssemblyAI websocket
+                    await ws.send(ws_data_packet.get('data'))
+                except ConnectionClosedError as e:
+                    logger.error(f"Connection closed while sending data: {e}")
+                    break
+                except Exception as e:
+                    logger.error(f"Error sending data to websocket: {e}")
+                    break
+                    
+        except asyncio.CancelledError:
+            logger.info("Sender stream task cancelled")
+            raise
+        except Exception as e:
+            logger.error('Error in sender_stream: ' + str(e))
+            raise
+
+    async def receiver(self, ws: ClientConnection):
+        async for msg in ws:
+            try:
+                msg = json.loads(msg)
+
+                # If connection_start_time is None, it is the durations of frame submitted till now minus current time
+                if self.connection_start_time is None:
+                    self.connection_start_time = (time.time() - (self.num_frames * self.audio_frame_duration))
+
+                if msg["message_type"] == "SessionBegins":
+                    logger.info("Received SessionBegins event from AssemblyAI")
+                    yield create_ws_data_packet("speech_started", self.meta_info)
+
+                elif msg["message_type"] == "PartialTranscript":
+                    transcript = msg["text"]
+
+                    if transcript.strip():
+                        data = {
+                            "type": "interim_transcript_received",
+                            "content": transcript
+                        }
+                        yield create_ws_data_packet(data, self.meta_info)
+
+                elif msg["message_type"] == "FinalTranscript":
+                    transcript = msg["text"]
+                    if transcript.strip():
+                        logger.info(f"Received final transcript - {transcript}")
+                        data = {
+                            "type": "transcript",
+                            "content": transcript
+                        }
+                        yield create_ws_data_packet(data, self.meta_info)
+
+                elif msg["message_type"] == "SessionTerminated":
+                    logger.info(f"Received SessionTerminated from AssemblyAI - {msg}")
+                    yield create_ws_data_packet("transcriber_connection_closed", self.meta_info)
+                    return
+
+            except Exception as e:
+                traceback.print_exc()
+                self.interruption_signalled = False
+
+    async def push_to_transcriber_queue(self, data_packet):
+        await self.transcriber_output_queue.put(data_packet)
+
+    async def assemblyai_connect(self):
+        """Establish websocket connection to AssemblyAI with proper error handling"""
+        try:
+            websocket_url = self.get_assemblyai_ws_url()
+            additional_headers = {
+                'Authorization': f'{self.api_key}'
+            }
+            
+            logger.info(f"Attempting to connect to AssemblyAI websocket: {websocket_url}")
+            
+            assemblyai_ws = await asyncio.wait_for(
+                websockets.connect(websocket_url, additional_headers=additional_headers),
+                timeout=10.0  # 10 second timeout
+            )
+            
+            self.websocket_connection = assemblyai_ws
+            self.connection_authenticated = True
+            logger.info("Successfully connected to AssemblyAI websocket")
+            
+            return assemblyai_ws
+            
+        except asyncio.TimeoutError:
+            logger.error("Timeout while connecting to AssemblyAI websocket")
+            raise ConnectionError("Timeout while connecting to AssemblyAI websocket")
+        except InvalidHandshake as e:
+            logger.error(f"Invalid handshake during AssemblyAI websocket connection: {e}")
+            raise ConnectionError(f"Invalid handshake during AssemblyAI websocket connection: {e}")
+        except ConnectionClosedError as e:
+            logger.error(f"AssemblyAI websocket connection closed unexpectedly: {e}")
+            raise ConnectionError(f"AssemblyAI websocket connection closed unexpectedly: {e}")
+        except Exception as e:
+            logger.error(f"Unexpected error connecting to AssemblyAI websocket: {e}")
+            raise ConnectionError(f"Unexpected error connecting to AssemblyAI websocket: {e}")
+
+    async def run(self):
+        try:
+            self.transcription_task = asyncio.create_task(self.transcribe())
+        except Exception as e:
+            logger.error(f"not working {e}")
+
+    def __calculate_utterance_end(self, data):
+        utterance_end = None
+        if 'words' in data:
+            if data['words']:
+                final_word = data['words'][-1]
+                utterance_end = self.connection_start_time + final_word['end']
+                logger.info(f"Final word ended at {utterance_end}")
+        return utterance_end
+
+    def __set_transcription_cursor(self, data):
+        if 'words' in data:
+            if data['words']:
+                final_word = data['words'][-1]
+                self.transcription_cursor = final_word['end']
+        logger.info(f"Setting transcription cursor at {self.transcription_cursor}")
+        return self.transcription_cursor
+
+    def __calculate_latency(self):
+        if self.transcription_cursor is not None:
+            logger.info(f'audio cursor is at {self.audio_cursor} & transcription cursor is at {self.transcription_cursor}')
+            return self.audio_cursor - self.transcription_cursor
+        return None
+
+    async def transcribe(self):
+        assemblyai_ws = None
+        try:
+            start_time = time.perf_counter()
+            
+            try:
+                assemblyai_ws = await self.assemblyai_connect()
+            except (ValueError, ConnectionError) as e:
+                logger.error(f"Failed to establish AssemblyAI connection: {e}")
+                await self.toggle_connection()
+                return
+            
+            if not self.connection_time:
+                self.connection_time = round((time.perf_counter() - start_time) * 1000)
+
+            if self.stream:
+                self.sender_task = asyncio.create_task(self.sender_stream(assemblyai_ws))
+                self.heartbeat_task = asyncio.create_task(self.send_heartbeat(assemblyai_ws))
+                
+                try:
+                    async for message in self.receiver(assemblyai_ws):
+                        if self.connection_on:
+                            await self.push_to_transcriber_queue(message)
+                        else:
+                            logger.info("closing the assemblyai connection")
+                            await self._close(assemblyai_ws, data={"type": "CloseStream"})
+                            break
+                except ConnectionClosedError as e:
+                    logger.error(f"AssemblyAI websocket connection closed during streaming: {e}")
+                except Exception as e:
+                    logger.error(f"Error during streaming: {e}")
+                    raise
+            else:
+                async for message in self.sender():
+                    await self.push_to_transcriber_queue(message)
+
+        except (ValueError, ConnectionError) as e:
+            logger.error(f"Connection error in transcribe: {e}")
+            await self.toggle_connection()
+        except Exception as e:
+            logger.error(f"Unexpected error in transcribe: {e}")
+            await self.toggle_connection()
+        finally:
+            if assemblyai_ws is not None:
+                try:
+                    await assemblyai_ws.close()
+                    logger.info("AssemblyAI websocket closed in finally block")
+                except Exception as e:
+                    logger.error(f"Error closing websocket in finally block: {e}")
+                finally:
+                    self.websocket_connection = None
+                    self.connection_authenticated = False
+            
+            if hasattr(self, 'sender_task') and self.sender_task is not None:
+                self.sender_task.cancel()
+            if hasattr(self, 'heartbeat_task') and self.heartbeat_task is not None:
+                self.heartbeat_task.cancel()
+            
+            await self.push_to_transcriber_queue(
+                create_ws_data_packet("transcriber_connection_closed", getattr(self, 'meta_info', {}))
+            )
